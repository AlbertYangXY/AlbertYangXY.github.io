# Ansible 部署ceph集群 #

2020/3/23 10:14:57 

----------

## 1.安装 ##

### GitHub ###

#### 克隆存储库： ####
    $ git clone https://github.com/ceph/ceph-ansible.git
    $ git checkout $branch

#### barnch说明： ####
- stable-3.0支持Ceph版本jewel和luminous。该分支需要Ansible版本2.4。
- stable-3.1支持Ceph版本luminous和mimic。该分支需要Ansible版本2.4。
- stable-3.2支持Ceph版本luminous和mimic。该分支需要Ansible版本2.6。
- stable-4.0支持Ceph版本nautilus。该分支需要Ansible版本2.8。
- master支持Ceph的master分支。该分支需要Ansible版本2.8。

#### 安装ansible和依赖(如果ansible存在则可注释掉ansible的安装)： ####
    $ pip install -r requirements.txt

## 2.配置与使用 ##
### inventory 文件： ###
    [mons]
    mon1 ansible_host=10.142.96.71
    mon2 ansible_host=10.142.96.72
    mon3 ansible_host=10.142.96.73
    [mgrs]
    mgr1 ansible_host=10.142.96.71
    mgr2 ansible_host=10.142.96.72
    mgr3 ansible_host=10.142.96.73
    [osds]
    osd1 ansible_host=10.142.96.71
    osd2 ansible_host=10.142.96.72
    osd3 ansible_host=10.142.96.73
    osd4 ansible_host=10.142.96.135
    osd5 ansible_host=10.142.96.136
    
### playbook剧本文件 ###
- 重命名示例剧本： mv site.yml.sample site.yml
- 根据集群的需要修改剧本

### 配置验证 ###
ceph-ansible项目通过ceph-validate角色提供了配置验证。

该ceph-validate角色当前支持以下osd方案的正确配置验证：

- collocated
- non-collocated
- lvm

以下安装选项也由ceph-validate角色验证:

- ceph_origin set to distro
- ceph_origin set to repository
- ceph_origin set to local
- ceph_repository set to rhcs
- ceph_repository set to dev
- ceph_repository set to community

### 安装方法 ###
以下是通过不同渠道安装Ceph的所有可用选项。

主要支持三种安装方法，全部由ceph_origin变量管理：

- repository：意味着您将通过新的存储库安装Ceph。后来以下之间选择community，rhcs或dev。这些选项将通过ceph_repository变量公开。
- distro：表示不会添加单独的repo文件，并且您将获得Linux发行版中包含的Ceph任何版本。
- local：表示将从本地计算机复制Ceph二进制文件（未经良好测试，使用后果自负）

#### 来源：Repository ####
如果ceph_origin设置为repository，您现在可以在由ceph\_repository选项控制的两个存储库之间进行选择：

- community：从官方社区Ceph仓库http://download.ceph.com获取程序包
- rhcs：表示您是Red Hat客户，此外，您还必须通过ceph\_repository_type（cdn或iso）选择存储库类型
- dev：从基于gitbuilder的软件包系统萨满中获取软件包
- uca：从Ubuntu Cloud Archive获取软件包
- custom：从特定存储库获取软件包
#### 社区存储库 ####
如果ceph\_repository设置为community，则默认情况下将从http://download.ceph.com安装您的软件包，可以通过调整进行更改ceph\_mirror。最后一步是选择要安装的Ceph版本，为此您必须进行相应设置ceph\_stable\_release。例如，。ceph\_stable\_release: luminous

#### RHCS库 ####
RHCS是来自Red Hat（Ceph的企业版）的Red Hat Ceph Storage产品。如果ceph_repository设置为rhcs，将从Red Hat来源安装软件包。

此外，您将必须通过选择存储库类型ceph\_repository\_type，可以是cdn或iso。要选择特定版本的RHCS，您可以相应地设置ceph\_rhcs\_version变量，例如：。ceph\_rhcs\_version: 2

#### UCA资料库 ####
如果ceph\_repository将设置为uca，则默认情况下将从http://ubuntu-cloud.archive.canonical.com/ubuntu安装您要安装的软件包，可以通过进行调整来进行更改ceph\_stable\_repo\_uca。您还可以通过调整来确定Ceph软件包应来自哪个OpenStack版本ceph\_stable\_openstack\_release\_uca。例如，。ceph\_stable\_openstack\_release\_uca: queens

#### 开发仓库 ####
如果ceph_repository设置为dev，则默认情况下将从https://shaman.ceph.com/安装您的软件包，无法进行调整。您显然可以借助来决定要安装哪个分支 ceph\_dev\_branch（默认为'master'）。此外，您可以使用来指定SHA1 ceph\_dev\_sha1，默认为“最新”（与最新构建的一样）。

#### 自定义存储库 ####
如果ceph\_repository设置为custom，则默认情况下将从所需的存储库安装软件包。该存储库指定了ceph\_custom\_repo，例如：。ceph\_custom\_repo: https://server.domain.com/ceph-custom-repo

### 来源：发行版 ###
如果ceph_origin设置为distro，则不会添加单独的回购文件，并且您将获得Linux发行版中包含的Ceph版本。

### 来源：本地 ###
如果ceph_origin设置为local，则将从本地计算机复制ceph二进制文件（未经良好测试，使用后果自负）

## 3.配置 ##
您的Ceph集群的配置将通过使用ceph-ansible提供的ansible变量进行设置。所有这些选项及其默认值都在项目group_vars/根目录下的目录中定义ceph-ansible。Ansible将在group_vars/与您的清单文件或剧本相关的目录中使用配置。在group_vars/目录内部，有许多示例Ansible配置文件，它们通过文件名与每个Ceph守护程序组相关。例如，osds.yml.sample包含OSD守护程序的所有默认配置。该all.yml.sample 文件是一个特殊group_vars文件，适用于群集中的所有主机。

在最基本的级别上，您必须告诉ceph-ansible您要安装的Ceph版本，安装方法，群集网络设置以及如何配置OSD。要开始配置，请重命名group_vars/您要使用的每个文件，以使其不包含.sample 在文件名末尾，请取消注释要更改的选项并提供您自己的值。

octopus使用lvm batch方法部署Ceph 上游版本的示例配置如下所示group_vars/all.yml：

    ceph_origin: repository
    ceph_repository: community
    ceph_stable_release: octopus
    public_network: "192.168.3.0/24"
    cluster_network: "192.168.4.0/24"
    monitor_interface: eth1
    devices:
      - '/dev/sda'
      - '/dev/sdb'

在所有安装中都需要更改以下配置选项，但是可能还有其他必需的选项，具体取决于您的OSD方案选择或群集的其他方面。

- ceph_origin
- ceph\_stable\_release
- public_network
- monitor\_interface 要么 monitor\_address

部署RGW实例时，需要设置radosgw\_interface或radosgw\_addressconfig选项。

### ceph.conf配置文件 ###
支持的定义您的方法ceph.conf是使用ceph\_conf\_overrides变量。这使您可以使用INI格式指定配置选项。此变量可用于覆盖已经在中定义的部分ceph.conf（请参阅：）roles/ceph-config/templates/ceph.conf.j2或提供新的配置选项。

#### ceph.conf支持以下各节： ####

- [global]
- [mon]
- [osd]
- [mds]
- [client.rgw.{instance_name}]

一个例子：

    ceph_conf_overrides:
       global:
     foo: 1234
     bar: 5678
       osd:
     osd_mkfs_type: ext4
     
以下各节中提供了有关配置每种Ceph守护程序类型的完整文档。
#### OSD配置 ####
过去通过选择OSD方案并提供该方案所需的配置来设置OSD配置。从稳定版4.0中的nautilus开始，唯一可用的方案是lvm。

### LVM ###
这个OSD场景使用Ceph -volume来创建OSD，主要使用LVM，并且只有当Ceph版本是luminous或者更新的时候才可用。它是自动启用的。

其他(可选)支持的设置:

- osd_objectstore:为OSD设置Ceph objectstore。可用的选项有filestore或bluestore。你只能选择bluestore与Ceph发布是luminous或更新。如果未设置，则默认为bluestore。
- dmcrypt:使用dmcrypt在OSDs上启用Ceph的加密。
- 如果未设置，则默认为false。
- osds\_per\_device:为每个设备提供超过1个OSD(未设置时的默认值)。

### 简单配置 ###
使用这种方法，关于设备如何配置以提供OSD的大部分决策都是由Ceph工具做出的(本例中是ceph-volume lvm批处理)。在给定设备输入的情况下，几乎没有修改OSD组成方式的空间。
要使用此配置，必须使用将用于供应OSDs的原始设备路径填充设备选项。

> 注意:
> 原始设备必须是“干净的”，没有gpt分区表或逻辑卷。

例如，对于具有/dev/sda和/dev/sdb的节点，其配置为:

    devices:
      - /dev/sda
      - /dev/sdb

在上面的例子中，如果两个设备都是旋转驱动器，就会创建两个osd，每个osd都有自己的配置日志。

其他供应策略是可能的，通过混合纺纱和固态设备，例如:

    devices:
      - /dev/sda
      - /dev/sdb
      - /dev/nvme0n1

与最初的示例类似，这将最终产生两个osd，但是数据将放置在较慢的旋转驱动器(/dev/sda，和/dev/sdb)上，而日志将放置在较快的固态设备/dev/nvme0n1上ceph-volume工具在“batch”子命令部分详细描述了这一点

这个选项也可以与osd\_auto\_discovery一起使用，这意味着您不需要直接填充设备，ansible找到的任何适当的设备都将被使用。

    osd_auto_discovery: true
    
其他(可选)支持的设置:

crush\_device\_class:为使用此方法创建的所有osd设置挤压设备类(使用简单配置方法不可能有每个osd挤压设备类)。值必须是字符串，如crush\_device\_class: "ssd"

### 高级配置 ###
当在设置设备时需要更细粒度的控制以及如何安排它们以提供OSD时，这种配置非常有用。它需要现有的卷组和逻辑卷设置(ceph-volume不会创建这些)。
要使用此配置，必须使用逻辑卷和卷组填充lvm_volumes选项。此外，绝对路径的分区可以用于journal, block.db和block.wal

> 注意:
> 此配置使用ceph-volume lvm创建来供应OSDs
### 支持的lvm_volumes配置设置： ###

- data：逻辑卷名称或原始设备的完整路径（将使用原始设备的100％创建LV）
- data_vg：卷组名，如果是逻辑卷则为必需data。
- crush\_device\_class：生成的OSD的CRUSH设备类名称，允许设置为每个OSD设置设备类，这与crush\_device\_class 为所有OSD 设置设备类的全局设置不同。

> 注意：如果要crush\_device\_class在使用时为OSD 设置OSD，则devices必须使用全局crush\_device\_class 选项进行设置，如上所示。devices像for那样使用时，无法为每个OSD定义特定的CRUSH设备类lvm_volumes。

#### filestore 对象库变量： ####

- journal：逻辑卷名或分区的完整路径。
- journal_vg：卷组名，如果是逻辑卷则为必需journal。

> 警告:
> 每个条目必须唯一，不允许重复值

#### bluestore 对象库变量： ####

- db：逻辑卷名或分区的完整路径。
- db_vg：卷组名，如果是逻辑卷则为必需db。
- wal：逻辑卷名或分区的完整路径。
- wal_vg：卷组名，如果是逻辑卷则为必需wal。

> 注意
> 这些bluestore变量是可选的优化。Bluestore的 db，wal只会从速度更快的设备中受益。可以使用单个原始设备创建一个蓝存储OSD。

> 警告
> 每个条目必须唯一，不允许重复值

#### bluestore 使用原始设备的示例： ####

    osd_objectstore: bluestore
    lvm_volumes:
      - data: /dev/sda
      - data: /dev/sdb
> 注意
> 在这种情况下，将使用100％的设备创建卷组和逻辑卷。

#### bluestore 逻辑卷示例： ####

    osd_objectstore: bluestore
    lvm_volumes:
      - data: data-lv1
    	data_vg: data-vg1
      - data: data-lv2
    	data_vg: data-vg2
> 注意
> 卷组和逻辑卷必须存在。

#### bluestore定义wal和db逻辑卷示例： ####

    osd_objectstore: bluestore
    lvm_volumes:
      - data: data-lv1
    	data_vg: data-vg1
    	db: db-lv1
    	db_vg: db-vg1
    	wal: wal-lv1
    	wal_vg: wal-vg1
      - data: data-lv2
    	data_vg: data-vg2
    	db: db-lv2
    	db_vg: db-vg2
    	wal: wal-lv2
    	wal_vg: wal-vg2
> 注意
> 卷组和逻辑卷必须存在。

#### filestore 逻辑卷示例： ####

    osd_objectstore: filestore
    lvm_volumes:
      - data: data-lv1
    	data_vg: data-vg1
    	journal: journal-lv1
    	journal_vg: journal-vg1
      - data: data-lv2
    	data_vg: data-vg2
    	journal: journal-lv2
    	journal_vg: journal-vg2
> 注意
> 卷组和逻辑卷必须存在。